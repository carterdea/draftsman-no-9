# Docker Execution Plan

Scope: Docker only. Control plane always-on. Job runner ephemeral.

## Decisions

- Control plane on always-on Mac now.
- Same compose shape later on Linux host.
- One queue system, not one queue per container.
- One ephemeral runner container per job.
- Worker orchestrates jobs; job logic runs inside runner container.
- Local-first backend is `docker run`; keep a runner interface so backend can swap later.
- Runner uses deterministic checkout (`git clone` + checkout exact SHA), not `git pull`.

## Tasks

- [ ] Create `docker-compose.yml` for `api`, `worker`, `redis`, `postgres`.
- [ ] Add healthchecks for every long-lived service.
- [ ] Set `restart: unless-stopped` on long-lived services.
- [ ] Add `.env.example` with non-secret defaults.
- [ ] Add `docker compose up -d` + recovery commands to `README.md`.
- [ ] Add runner contract doc (`image`, env vars, workspace mount, exit codes).
- [ ] Add runner interface in code (`start`, `status`, `cancel`, `logs`) with `DockerRunner` as v1.
- [ ] Add cleanup policy for dead containers/volumes/log growth.
- [ ] Add Mac host hardening notes (disable sleep, auto-restart Docker Desktop).
- [ ] Add explicit timeout + cancel behavior (`docker stop` then forced kill fallback).
- [ ] Ensure only control plane has Docker socket access in local dev; never mount socket in runner.
- [ ] Create dependency cache volumes (bun, npm, bundler, uv, pip) and add monthly prune cron.
- [ ] Implement `repo_profiles` table (see schema above) and CRUD endpoints.
- [ ] Implement Dockerfile resolution logic (use repo's Dockerfile or generate from profile + version hints).
- [ ] Implement image build + cache logic (tag by Dockerfile hash, skip build on cache hit).
- [ ] Wire worker to resolve image → build if needed → `docker run` with correct cache volumes per job.
- [ ] Smoke test on Mac host (cold start, internet drop, reboot).

## `docker-compose.yml` spec (v1)

```yaml
services:
  api:
    build:
      context: .
      dockerfile: apps/api/Dockerfile
    command: bun run start
    env_file: .env
    ports:
      - "3000:3000"
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:3000/health"]
      interval: 15s
      timeout: 3s
      retries: 5

  worker:
    build:
      context: .
      dockerfile: apps/worker/Dockerfile
    command: bun run start
    env_file: .env
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bun", "run", "healthcheck"]
      interval: 15s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: draftsman
      POSTGRES_USER: draftsman
      POSTGRES_PASSWORD: draftsman
    volumes:
      - pg_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U draftsman -d draftsman"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  redis_data:
  pg_data:
  draftsman_bun_cache:
  draftsman_npm_cache:
  draftsman_bundler_cache:
  draftsman_uv_cache:
  draftsman_pip_cache:
```

## Runner image strategy (per-repo Dockerfile)

Every target repo gets its own runner image built from a Dockerfile. This handles the real-world problem: different projects need different toolchain versions (Ruby 3.2 vs 3.3, Node 18 vs 22, etc.). A small set of ecosystem images can't solve this without multi-version managers inside the image, which is the complexity we're trying to avoid.

### Resolution order

1. **Repo has a Dockerfile** — use it directly. The project team already solved this problem. Draftsman builds from their Dockerfile, mounts cache volumes, clones into `/workspace`. The setup profile points at the Dockerfile path (e.g. `Dockerfile`, `Dockerfile.dev`, `.devcontainer/Dockerfile`).

2. **Repo has no Dockerfile** — Draftsman generates one from the setup profile + version hints found in the repo (`.ruby-version`, `.node-version`, `.python-version`, `package.json#engines`, etc.). The generated Dockerfile is stored in the `repo_profiles` table alongside the setup profile. It becomes a reviewable, editable artifact. Next run reuses it unless the profile or version hints change.

### Generated Dockerfile strategy

Generated Dockerfiles are minimal and predictable:

```dockerfile
# Auto-generated by Draftsman from setup profile
# Repo: owner/repo | Profile hash: abc123
FROM node:22-slim

RUN apt-get update && apt-get install -y git curl jq && rm -rf /var/lib/apt/lists/*

# Bun (if declared in profile)
RUN curl -fsSL https://bun.sh/install | bash
ENV PATH="/root/.bun/bin:$PATH"

WORKDIR /workspace
```

Base image selection uses version hints:

| Version hint source | Maps to |
|---|---|
| `.node-version`, `package.json#engines.node` | `node:<version>-slim` |
| `.ruby-version`, `Gemfile` ruby constraint | `ruby:<version>-slim` |
| `.python-version`, `pyproject.toml` requires-python | `python:<version>-slim` |
| No hints found | `debian:bookworm-slim` + explicit installs |

### Image caching

Built images are tagged `draftsman-runner:<owner>-<repo>:<dockerfile-hash>` and cached locally. The hash is computed from the Dockerfile contents. A matching tag means the image is already built — skip the build, go straight to `docker run`. First-run cold build is the only real cost; subsequent jobs reuse the cached image.

Rebuild triggers:
- Dockerfile contents changed (hash mismatch)
- Setup profile updated (regenerates Dockerfile for generated ones)
- Manual rebuild requested via admin dashboard
- Weekly staleness check (configurable, default 7 days)

## Dependency cache volumes

Package manager caches are host-side Docker volumes, mounted read-write into every runner container.

| Dependency manager | Cache path in container | Volume name |
|---|---|---|
| bun | `/.bun/install/cache` | `draftsman_bun_cache` |
| npm/yarn | `/.npm` | `draftsman_npm_cache` |
| bundler | `/usr/local/bundle/cache` | `draftsman_bundler_cache` |
| uv | `/.cache/uv` | `draftsman_uv_cache` |
| pip | `/.cache/pip` | `draftsman_pip_cache` |

The per-repo setup profile declares which dependency manager the repo uses. The worker mounts only the relevant cache volume(s).

Cache volumes are content-addressed by design (bun, uv, pip all deduplicate internally), so no per-repo isolation is needed. Add a monthly prune cron for hygiene:

```bash
# Prune cache volumes older than 30 days (safe — packages re-download on next miss)
docker volume ls -q -f name=draftsman_*_cache | xargs -I{} docker run --rm -v {}:/cache alpine find /cache -atime +30 -delete
```

## Per-repo setup profile contract

The README references "setup profiles" — this is the concrete contract. Each target repo has a profile that tells the runner how to set it up and validate it.

```json
{
  "repo": "owner/repo",
  "dockerfile": "Dockerfile.dev",
  "dependency_manager": "bun",
  "setup": "bun install",
  "validation": {
    "fast": ["bun run lint", "bun run typecheck"],
    "full": ["bun test"]
  }
}
```

Rules:
- `dockerfile` points at an existing Dockerfile in the repo. If `null`, Draftsman generates one (see runner image strategy above). The generated Dockerfile is stored in the `generated_dockerfile` column of the `repo_profiles` table.
- `dependency_manager` selects which cache volumes to mount.
- `setup` runs once after checkout. Uses the same install command a human developer would run.
- `validation.fast` runs every Ralph Loop iteration. Should complete in seconds.
- `validation.full` runs only when fast checks pass, capped at 2–3 runs per job. Typically the test suite.
- All commands are the same ones humans use locally. No agent-specific wrappers.

Profiles are stored in Postgres (`repo_profiles` table) and editable via the admin dashboard.

### `repo_profiles` table shape

```sql
CREATE TABLE repo_profiles (
  id            UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  repo          TEXT NOT NULL UNIQUE,       -- 'owner/repo'
  dockerfile    TEXT,                        -- path in repo, NULL = generate
  generated_dockerfile TEXT,                 -- stored generated Dockerfile content
  dockerfile_hash TEXT,                      -- hash for image cache key
  dependency_manager TEXT NOT NULL,          -- 'bun' | 'npm' | 'bundler' | 'uv' | 'pip'
  setup_command TEXT NOT NULL,               -- 'bun install', 'bundle install', etc.
  validation_fast TEXT[] NOT NULL,           -- fast check commands
  validation_full TEXT[] NOT NULL,           -- full check commands
  created_at    TIMESTAMPTZ DEFAULT now(),
  updated_at    TIMESTAMPTZ DEFAULT now()
);
```

## Ephemeral runner pattern (non-compose)

- Worker resolves the runner image for the repo (build from Dockerfile if needed, or use cached image).
- Worker launches per-job container with `--rm`.
- Mount temp workspace + relevant dependency cache volume(s).
- Env: scoped GitHub token, job id, repo, commit SHA.
- No prod creds. No docker socket in runner.
- Kill on timeout. Upload logs/artifacts. Exit.
- Job flow in runner: clone repo → checkout exact SHA → restore dependency cache → run setup → run bounded Ralph loop (with tiered validation) → return structured result.

### Worker job flow (image resolution)

```
1. Look up repo_profiles for the target repo
2. If dockerfile is set:
     - Clone repo, read Dockerfile at that path
     - Hash contents → check for cached image tag
     - If cache miss → docker build
3. If dockerfile is null:
     - Check generated_dockerfile column
     - If null or stale → generate from profile + repo version hints
     - Hash contents → check for cached image tag
     - If cache miss → docker build from generated Dockerfile
4. docker run with resolved image
```

### Example run shape

```bash
# Image was resolved in step above
IMAGE_TAG="draftsman-runner:acme-shopify-app:a1b2c3"

docker run --rm \
  --name draftsman-job-$JOB_ID \
  --network draftsman_default \
  -e JOB_ID \
  -e GITHUB_TOKEN \
  -e REPO \
  -e COMMIT_SHA \
  -v /tmp/draftsman/$JOB_ID:/workspace \
  -v draftsman_bun_cache:/.bun/install/cache \
  $IMAGE_TAG
```

## Ops notes

- Mac host: prevent sleep, enable Docker Desktop auto-start.
- Daily cleanup cron: prune old images/volumes with safe age filter.
- Backups: Postgres volume snapshot schedule.
- Logs: keep size caps to avoid disk fill.

## Done looks like

- `docker compose up -d` survives reboot + transient internet loss.
- `api` + `worker` auto-recover without manual steps.
- Job container starts <10s, exits clean, leaves audit trail.
